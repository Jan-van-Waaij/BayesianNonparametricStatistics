\documentclass[12pt]{article}
\usepackage{jansstylefile} % The option nobibliography does not print the bibliography. But bibliography can still be used. 

\title{Posterior distribution}
\author{Jan van Waaij}

\begin{document}

\section{Distribution of the posterior of a finite basis expansion with Gaussian coefficients}

\begin{lemma}\label{lem:posteriordistribution}
	Let \(X^T=\rh{X_t:t\in[0,T]}\) be an observation of 
	\begin{align*}
		dX_t=b(X_t)dt+\sigma(X_t)dW_t,
	\end{align*}
	where \(b\) is equipped with the prior distribution defined by 
	\[
	b=\sum_{j=1}^k\theta_j\phi_j,
	\]
	where \(\set{\phi_1,\ldots,\phi_k}\) is a linearly independent basis, and \(\theta=(\theta_1,\ldots,\theta_k)^t\) has multivariate normal distribution \(N(\mu,\Sigma)\), with mean vector $\mu$ and positive definite matrix $\Sigma$, and \(\sigma:\re\to \re_{>0}\) is a  measurable function. Then the  posterior distribution of \(\theta\) given $X^T$ is \(N(\hat\mu,\hat\Sigma)\), where \[\hat\mu=(S+\Sigma^{-1})^{-1}(m+\Sigma^{-1}\mu),\quad\hat\Sigma= (S+\Sigma^{-1})^{-1}\] and the vector \(m=(m_1,\ldots,m_k)^t\) is defined by 
	\[
	m_l=\int_0^T\frac{\phi_l(X_t)}{\sigma(X_t)^2}dX_t, \quad l=1,\ldots,k,
	\] 
	and the symmetric \(k\times k\)-matrix \(S\) is given by 
	\begin{equation}\label{eq:girsanovmatrix}
	S_{l,l'}=\int_0^T\frac{\phi_l(X_t)\phi_{l'}(X_t)}{\sigma^2(X_t)}dt,\quad l,l'=1,\ldots,k,
	\end{equation}
	provided \(S+\Sigma^{-1}\) is invertible. 
	Moreover, the marginal likelihood is given by 
	\[
\int p(X^T\mid \theta)p(\theta)d\theta=	|\Sigma^{-1}\hat\Sigma|^{1/2}e^{-\frac12\mu^t\Sigma^{-1}\mu} e^{\frac12\hat\mu^t\hat\Sigma^{-1}\hat\mu}.
	\]
	
\end{lemma}
\begin{proof}Almost surely we have by Girsanov's theorem (e.g. \cite[chapter 13]{steele2001} or \cite[section 9.4]{ChungWilliams2014}) \begin{equation}\label{eq:girsanov}
p(X^T\mid \theta)=\exp\left(\int_0^T\frac{b(X_t)}{\sigma(X_t)^2}dX_t-\frac12\int_0^T\rh{\frac{b(X_t)}{\sigma(X_t)}}^2dt\right),
\end{equation}with respect to the Wiener measure. So \begin{equation}\label{eq:loglikelihoodintermsofmandS}
\log p(X^T\mid b)=\theta^tm - \frac 1 2 \theta^t S\theta
\end{equation}
 and the log of the distribution of \(\theta\) with respect to the Lebesgue measure on \(\re^k\) is given by
\begin{align*}
	\log p(\theta)= &-\frac k2\log(2\pi) - \frac12 \log|\Sigma|  - \frac 1 2 (\theta-\mu)^t\Sigma^{-1}(\theta-\mu) \\
	= &C_1 - \frac 1 2 \theta\Sigma^{-1}\theta +\theta^t\Sigma^{-1}\mu,
	\intertext{with}
	C_1=& -\frac k2\log(2\pi) - \frac12 \log|\Sigma|  - \frac 1 2 \mu^t\Sigma^{-1}\mu. 
\end{align*}


So, %by the Bayes formula, for some constant \(C_3\), the posterior density of \(\theta\) is given by
\begin{align*}
	\log( p(X^T\mid \theta)p(\theta)) = & C_1 + \theta^tm - \frac 1 2 \theta^t S\theta - \frac 1 2 \theta\Sigma^{-1}\theta +\theta^t\Sigma^{-1}\mu\\
	= & C_1 + \theta^t ( m + \Sigma^{-1} \mu ) - \frac 1 2 \theta^t (S+\Sigma^{-1}) \theta\\
	= & C_1 + \theta^t ( S + \Sigma^{-1} )  \Big  ( ( S + \Sigma^{-1} )^{-1} (m + \Sigma^{-1}\mu )\Big) \\
	&\quad- \frac 1 2 \theta^t (S+\Sigma^{-1}) \theta. 
\end{align*}

By the Bayes formula, the posterior density of \(\theta\) is proportional to \(p(X^T\mid\theta)p(\theta)\). It follows that  \(\theta\mid X^T\) is normally distributed with mean 
\[\hat\mu :=( S + \Sigma^{-1} )^{-1} (m + \Sigma^{-1}\mu).\]
 and covariance matrix \[\hat\Sigma:=(S+\Sigma^{-1})^{-1},\]
provided $S+\Sigma^{-1}$ is invertible.  
 Moreover 
 \begin{align*}
 &\int  p(X^T\mid\theta)p(\theta)d\theta \\
 = & \int  e^{C_1} e^{\theta^t\hat\Sigma^{-1}\hat\mu } e^{-\frac12\theta^t \hat\Sigma^{-1} \theta} d\theta\\
 = & (2\pi)^{k/2}| \hat\Sigma|^{1/2}e^{\frac12\hat\mu^t\hat\Sigma^{-1}\hat\mu}e^{C_1}\\
 &\times \int (2\pi)^{-k/2}|\hat\Sigma|^{-1/2}e^{\theta^t\hat\Sigma^{-1}\hat\mu } e^{-\frac12\theta^t \hat\Sigma^{-1} \theta} e^{-\frac12\hat\mu^t\hat\Sigma^{-1}\hat\mu}d\theta\\
 = & (2\pi)^{k/2}|\hat\Sigma|^{1/2}e^{\frac12\hat\mu^t\hat\Sigma^{-1}\hat\mu}e^{C_1}\\
 = & |\Sigma^{-1}\hat\Sigma|^{1/2}e^{-\frac12\mu^t\Sigma^{-1}\mu} e^{\frac12\hat\mu^t\hat\Sigma^{-1}\hat\mu},
 \end{align*}
 using that the integrant in the third last line is the density of a multivariate normal distribution and therefore integrates to one.
\end{proof}

\section{The marginal maximum likelihood estimator}

\begin{lemma}\label{lem:marginallikelihood}
Let $\lambda>0$, $\mu\in\re^k$ and let $\Sigma$ be a positive definite $k\x k$-matrix. Consider the prior \(\theta\sim N(\mu,\Sigma_\lambda)\), where \(\Sigma_\lambda=\lambda^2\Sigma \) and denote its density by $p_\lambda$. Then 
\begin{equation}\label{eq:marginallikelihood}
\begin{split}
  &\log \int  p_\lambda(X^T\mid\theta)p_\lambda(\theta)d\theta  \\= &-\frac12\log |\lambda^2\Sigma  S + \II_k|  -\frac12\mu^t\Sigma^{-1}\mu+ \frac12(m + \lambda^{-2}\Sigma^{-1}\mu)^t ( S + \lambda^{-2}\Sigma^{-1} )^{-1} (m + \lambda^{-2}\Sigma^{-1}\mu).
\end{split}
\end{equation}

\end{lemma}
\begin{proof}It follows from \cref{lem:posteriordistribution} that 
  \[
\Sigma_\lambda \hat \Sigma_\lambda ^{-1} = \Sigma_\lambda (S+\Sigma_\lambda ^{-1})= \Sigma_\lambda  S + \II_k=\lambda^2\Sigma  S + \II_k
\]
and\begin{align*}
\hat \mu^t \hat\Sigma_\lambda^{-1} \hat\mu =&  (m + \Sigma_\lambda^{-1}\mu)^t( S + \Sigma_\lambda^{-1} )^{-1}( S + \Sigma_\lambda^{-1} )( S + \Sigma_\lambda^{-1} )^{-1} (m + \Sigma_\lambda^{-1}\mu)\\
=&  (m + \lambda^{-2}\Sigma^{-1}\mu)^t ( S + \lambda^{-2}\Sigma^{-1} )^{-1} (m + \lambda^{-2}\Sigma^{-1}\mu). 
\end{align*}
So it follows from the same lemma that 
\begin{align*}
    &\log \int  p_\lambda(X^T\mid\theta)p_\lambda(\theta)d\theta  \\= &-\frac12\log |\lambda^2\Sigma  S + \II_k|  -\frac12\mu^t\Sigma^{-1}\mu+ \frac12(m + \lambda^{-2}\Sigma^{-1}\mu)^t ( S + \lambda^{-2}\Sigma^{-1} )^{-1} (m + \lambda^{-2}\Sigma^{-1}\mu).
\end{align*}
\end{proof}

\section{Random scaling}

\begin{lemma}
	Let \(X^T=\rh{X_t:t\in[0,T]}\) be an observation of 
	\begin{align*}
	dX_t=b(X_t)dt+\sigma(X_t)dW_t,
	\end{align*}
	where \(b\) is equipped with the prior distribution defined by 
	\begin{align*}
	\lambda^2 \sim & \text{Inverse Gamma}(A,B)=IG(A,B)\\
	\theta \mid \lambda \sim & N(\mu,\lambda^2\Sigma)\\
	b\mid \theta = & \sum_{j=1}^k\theta_j\phi_j,
	\end{align*}
	where \(\set{\phi_1,\ldots,\phi_k}\) is a linearly independent basis. Then 
	\[
	\lambda^2 \mid \theta, X^T \sim \text{IG}\rh{ A + k/2 , B+ \frac12 (\theta-\mu)^t\Sigma^{-1}(\theta-\mu) }.
	\] 
\end{lemma}
\begin{proof}
%	Let where the vector \(m=(m_1,\ldots,m_k)^t\) is defined by 
%	\[
%	m_l=\int_0^T\frac{\phi_l(X_t)}{\sigma(X_t)^2}dX_t, \quad l=1,\ldots,k,
%	\] 
%	and the symmetric \(k\times k\)-matrix \(S\) is given by 
%	\[
%	S_{l,l'}=\int_0^T\frac{\phi_l(X_t)\phi_{l'}(X_t)}{\sigma^2(X_t)}dt,\quad l,l'=1,\ldots,k.
%	\]
%	Almost surely we have by Girsanov's theorem\begin{equation}\label{eq:girsanov}
%	p(X^T\mid b)=\exp\left(\int_0^T\frac{b(X_t)}{\sigma(X_t)^2}dX_t-\frac12\int_0^T\rh{\frac{b(X_t)}{\sigma(X_t)}}^2dt\right),
%	\end{equation}with respect to the Wiener measure. 
Recall \cref{eq:loglikelihoodintermsofmandS}, \(\log p(X^T\mid b)=\theta^tm - \frac 1 2 \theta^t S\theta\). The logarithm of the distribution of \(\theta\) given \(\lambda\) with respect to the Lebesgue measure on \(\re^k\) is given by (proportionality w.r.t. \(\lambda\)),
	\begin{align*}
	\log p(\theta\mid \lambda)= &C_1 -k\log \lambda - \frac 1 2 \lambda^{-2}(\theta-\mu)^t\Sigma^{-1}(\theta-\mu). 
	\end{align*}
	for some real constant \(C_1\), depending on \(\theta\), but not on \(\lambda\).
	
	In the following, \(\propto\) means equal up to a multiplicative constant depending on \(\theta\) and \(X^T\), but not on \(\lambda\).
	By the Bayes formula, \begin{align*}
		p(\lambda^2\mid \theta, X^T)\propto & p(X^T\mid \lambda^2,\theta)p(\lambda^2\mid \theta)
		\intertext{and}
		p(\lambda^2\mid \theta)\propto & p(\theta\mid \lambda^2)p(\lambda^2)\intertext{so}
		p(\lambda^2\mid \theta, X^T)\propto &  p(X^T\mid \lambda^2,\theta)p(\theta\mid \lambda^2)p(\lambda^2).
	\end{align*}It follows that for some real constants \(C,\tilde C\) depending on \(\theta\) and \(X^T\), but not on \(\lambda\), we have \begin{align*}
		&\log p(\lambda^2\mid \theta, X^T)\\ 
		= & C + \theta^tm - \frac 1 2\theta^t S\theta\\
		& -k\log \lambda - \frac12 \lambda^{-2}(\theta-\mu)^t\Sigma^{-1}(\theta-\mu)\\
		&-(A+1)\log(\lambda^2) - \frac B{\lambda^2}\\
		= &\tilde{C} -(A+k/2+1)\log(\lambda^2) - \frac {B+ \frac12 (\theta-\mu)^t\Sigma^{-1}(\theta-\mu)}{\lambda^2},
	\end{align*}
	which is up to an additive constant the logarithm of the density of the inverse gamma distribution with shape parameter \(A+k/2\) and scale parameter \(B+ \frac12 (\theta-\mu)^t\Sigma^{-1}(\theta-\mu)\).
\end{proof}

\begin{lemma}
	We have \begin{align*}
	&\log p(X^T\mid j,\lambda^2)\\
	=&-\frac12\log |\lambda^2\Sigma  S + \II_k|  -\frac12\mu^t\Sigma^{-1}\mu+ \frac12(m + \lambda^{-2}\Sigma^{-1}\mu)^t ( S + \lambda^{-2}\Sigma^{-1} )^{-1} (m + \lambda^{-2}\Sigma^{-1}\mu).
	\end{align*}
\end{lemma}
\begin{proof}
	This follows from \[
	p(X^T\mid j,\lambda^2 ) = \int p(X^T\mid j,\theta^j,\lambda^2)p(\theta^j\mid j, \lambda)d\theta^j
	\]
	and \cref{lem:marginallikelihood}. 
\end{proof}

\section{The sparsity of the Girsanov matrix with Faber-Schauder functions}

The Faber-Schauder basis functions $\psi_0, \psi_{j,k}$ are defined as follows: \begin{align*}
\psi_0(x)=&\begin{cases}
    1-2x & \text{ when } x\in[0,1/2),\\
    2x-1 & \text{ when } x\in[1/2,1],\\
    0 & \text{ otherwise,} 
\end{cases}\\
\Lambda(x) = & \begin{cases}
    2x & \text{ when }x\in[0,1/2),\\
    2(1-x) & \text{ when } x\in [1/2,1],\\
    0 & \text{ otherwise,}
\end{cases} 
\intertext{and}
\psi_{j,k} (x) = &  \Lambda(2^jx-k+1), \quad j=0,1,\ldots, k=1,\ldots,2^j,
\end{align*}
see \cite[p. 607]{meulenschauerwaaij2018}. We say that $\psi_0$ and $\psi_{0,1}$ are of level zero, and the basis functions $\psi_{j,1},\ldots,\psi_{j,2^j}$ are said to be of level $j$. The Girsanov matrix $S$ defined in \cref{eq:girsanovmatrix} with all basis function up to and including level $J$ is denoted by $S^J$. Note that $S^J$ has $2+\sum_{j=1}^J2^j=2^{J+1}$ rows and columns, and $2^{2J+2}$ entries. 

\begin{definition}
Let $M^n$ be an $n\times n$-matrix, and let $nz(M^n)$ the number of non-zero entries of $M^n$.
The level of sparsity of $M^n$ is the fraction of nonzero entries, $\frac{nz(M^n)}{n^2}$. 
\end{definition}

The definition of a sparse matrix is vague. Usually, we mean that the number of nonzero entries grows at most linear with the number of rows. We will establish that for $S^n$, the number of nonzero entries grows at most like $r\log r$ with $r$ the number of rows. 

Recall the definition of $S_{l,l'}$ in \cref{lem:marginallikelihood}.
Note that $S_{l,l'}=0$  when $\supp(\psi_l)\cap\supp(\psi_{l'})$ has Lebesgue measure zero. We say that $\psi_l$ and $\psi_{l'}$ have non-overlapping support when their supports are either disjoint or only share a boundary point; otherwise, we say they have overlapping support. 

%Note that for level \(j\ge 1\), \(\psi_{j,k}\) and \(\psi_{j,l}\) have non-overlapping support when \(k=l\) (obviously, then they are equal). 

Note that both functions of level zero,  \(\psi_1\) and \(\psi_{0,1}\), have the same support $[0,1]$. 

When $j\ge 0, d\ge 0$ and $d+j\ge 1$, there are \(2^d\) Faber functions of level \(j+d\) that have overlapping support with \(\psi_{j,k}\), \(j\ge 0\). These are 
\[
\psi_{j+d,(k-1)2^d+1},\psi_{j+d,(k-1)2^d+2},\ldots,\psi_{j+d,k2^d}
\]
For level 0, there are exactly two, and for level $1,\ldots,j-1$ there is precisely one basis function with overlapping support with $\psi_{j,k}$. 

So for $\psi_0$ and $\psi_{0,1}$ there are \begin{align*}
    2 + \sum_{d=1}^J 2^d = 2^{J+1} 
\end{align*}
basis functions $\psi_0,\psi_{j',k'}, j'\le J$ with overlapping support. 
For $\psi_{j,k}$, $j\ge 1$,  there are \begin{align*}
    2+ j-1 + \sum_{d=0}^{J-j} 2^d = %j+1 + 2^{J-j+1}-1=
    j+2^{J-j+1}
\end{align*}
basis functions $\psi_0,\psi_{j',k'}, j'\le J$, with overlapping support. When we make use of \cref{lem:sumjtwotothepowerj}, we see that $S^n$ has at most \begin{align*}
   &2\cdot 2^{J+1}+\sum_{ j=1}^J 2^j\rh{j+2^{J-j+1}}\\
= & 2\cdot 2^{J+1}+ (J-1)2^{J+1} + 2 + J2^{J+1}  \\
= & (2J+1)2^{J+1}+2
   %= 2^{J+1}+2^{J+1} + \sum_{ j=1}^J j2^j + J2^{J+1}\\ 
\end{align*}
nonzero entries. 

%Note that level zero has 2 basis functions, and level $j$, $j\ge 1$ has $2^j$ basis functions. In total there are \[
%2+ \sum_{j=1}^J2^j=%1+2^{J+1}-1=
%2^{J+1}
%\]
%basisfunctions. Hence, $S^J$ is a $2^{J+1}\x 2^{J+1}$-matrix with $2^{2J+2}$ entries. 
So the number of nonzero entries of $S^n$ grows at most like $r\log r$ with $r$ the number of rows. It has level of sparsity at most
\[
\frac{(2J+1)2^{J+1}+2
}{2^{2J+2}}= (2J+1)2^{-J-1}+2^{-2J-1},
\]
which is of the order $\frac r {\log r}$. 
\appendix

\section{Lemma}

\begin{lemma}\label{lem:sumjtwotothepowerj}
   For each $J\in \NN$,  \[
    \sum_{j=1}^Jj2^j=(J-1)2^{J+1} + 2. 
    \]
\end{lemma}
\begin{proof}
    Note that 
    \begin{align*}
    \sum_{j=1}^J j2^j =&\sum_{j=1}^J \sum_{k=j}^J2^k\\
    = & \sum_{j=1}^J 2^j\sum_{k=0}^{J-j}2^k \\
    = & \sum_{j=1}^J 2^j (2^{J-j+1}-1)\\
    = & J2^{J+1} - (2^{J+1}-2)\\  
    = & (J-1)2^{J+1} + 2.
\end{align*}

\end{proof}


%
%
%Every Faber-Schauder function is obviously dependent with itself. 
%
%Indexing with \(i=2^j+k\), when \(\psi_{j,k}\) has index \((j,k)\) (excluding \(i=1\)), we see that, when \(j\ge 0\), \(\psi_i\) is dependent with \(2^{j'-j}\), functions \(\psi_{j',k'}\), \(i'=2^{j'}+k'\ge i\) of level \(j'\ge j\)  (including itself, when \(j'=j\)).
%
%So if \(J\) is the higest level, \(\psi_i\) is dependent with 
%\[
%\sum_{d=0}^{J-j}2^d=2^{J-j+1}-1.
%\]
%Faber-Schauder functions \(\psi_{i'}\) with index \(i'\ge i\). 
%Hence summing over all levels \(0,\ldots,J\) and indices within a level, the number of combinations of functions \((\psi_{j,k},\psi_{j',k'}),0\le j,j'\le J\) and  \(i=2^j+k\le 2^{j'}+k'=i'\) which are dependent is
%\begin{align*}
%\sum_{j=0}^J\sum_{k=1}^{2^j}(2^{J-j+1}-1)\\
%=\sum_{j=0}^J(2^{J+1}-2^j)\\
%=(J+1)2^{J+1}-(2^{J+1}-1)\\
%=J2^{J+1}+1. 
%\end{align*}
%
%The Faber-Schauder function \(\psi_1\) is dependent with every Faber-Schauder function (including itself) up to and including level \(J\), which counts for \(2^{J+1}\) Faber-Schauder functions with a higher index or equal index, up to level \(J\). 
%
%In total we have 
%
%\[
%J2^{J+1}+1+2^{J+1}=(J+1)2^{J+1}+1. 
%\]
%Faber-Schauder functions up to level \(J\) dependent with a Faber-Schauder function with equal (itself) or higher index. 
%
%
%If we only consider dependent pairs \((\psi_i,\psi_{i'})\) with \(i'>i\), then we have 
%\[
%J2^{J+1}+1
%\]
%of such pairs (minus all \(2^{J+1}\) diagonal pairs \((\psi_i,\psi_i)\)).
%
%Hence, by symmetry, there are in total \(J2^{J+1}+1+J2^{J+1}+1+2^{J+1}=(2J+1)2^{J+1}+2\) pairs \((\psi_i,\psi_{i'})\) that are dependent.
%
%\begin{lemma}
%	The Girsanov covariantie matrix is sparse.
%\end{lemma}
%\begin{proof}
%	At most \((2J+1)2^{J+1}+2\) entries of the \(2^{J+1}\times 2^{J+1}\)-matrix (\(2^{2J+2}\) entries) are nonzero. The fraction of nonzero elements is at most
%	\[
%	\frac{(2J+1)2^{J+1}+2}{2^{2J+2}}=(2J+1)2^{-J-1}+2^{-2J-1},
%	\]
%	which converges to zero. 
%\end{proof}
\end{document}
